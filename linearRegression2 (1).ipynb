{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dae0ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%pylab inline\n",
    "\n",
    "m = 100;\n",
    "a = 0.5\n",
    "b = 2\n",
    "x = linspace(0,10,m)\n",
    "y = a * x + b + 0.3 * random.randn(m)\n",
    "plot(x, y, 'r+');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous assignment you implemented the cost function and one step of gradient descent. Now it's time to put your code together and implement to full gradient descent algorithm. Please reuse the implementations of your functions from the previous assignment, but make sure they are correct. \n",
    "\n",
    "Make sure to comment your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a prediction function h\n",
    "def predict(x,theta0,theta1):\n",
    "    x_predict = theta0+(theta1*x)\n",
    "    return x_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the output of that function to compute the cost function J:\n",
    "def cost(x_predict,y):\n",
    "    x = np.transpose(x)\n",
    "    y = np.transpose(y)\n",
    "    result = np.sum((x_predict-y)**2)\n",
    "    result = result/(2*len(x))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "def gradDescentStep(learningrate,x_predict,y,x):\n",
    "    dtheta0= sum(x_predict-y)/size(x)\n",
    "    dtheta1= (x_predict-y).dot(x)/size(x) #this has to be a scalar if you just multiply it it will be a vector, so use .dot()\n",
    "    theta0= theta0-learningrate*dtheta0\n",
    "    theta1= theta1-learningrate*dtheta1\n",
    "    return theta0, theta1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gradDescentStep() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-0404b6f29f2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstopcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mgradDescentStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxPrediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: gradDescentStep() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Repeat the gradient descent step untill it converges.\n",
    "# These are some default parameters, see how playing with them affects the behavior\n",
    "alpha = 0.1\n",
    "theta0 = 0\n",
    "theta1 = 1\n",
    "iterations = 10\n",
    "\n",
    "#Fill in the stopcondition yourself\n",
    "stopcondition = 0.01\n",
    "\n",
    "i = 0\n",
    "cost = 10\n",
    "#Try to save the output of the cost function at each iteration and plot it at the end\n",
    "while (i < iterations) and (costprevious - costcurrent > stopcondition):\n",
    "    i = i +1\n",
    "    gradDescentStep(alpha,xPrediction,y,x)\n",
    "    \n",
    "plt.plot(x,predict(x,theta0,theta1))\n",
    "plt.plot(x,y,'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Write a short analysis about the amount of iterations necessary to obtain a good result, the influence of the learning rate and the trend of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Problem: Implement Least Squares with closed form solution\n",
    "\n",
    "For the Least Squares method there is also a closed-form solution.\n",
    "\n",
    "$\\theta_1$ can be found by:\n",
    "$$ \\boldsymbol{\\hat\\theta_1} =( X ^TX)^{-1}X^{T}\\boldsymbol y $$\n",
    "\n",
    "You can leave $\\theta_0$ to be 0. Make a plot with your data as dots and your prediction as a line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
