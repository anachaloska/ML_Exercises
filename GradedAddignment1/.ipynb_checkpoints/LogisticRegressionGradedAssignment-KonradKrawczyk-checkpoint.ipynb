{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "# Graded Lab Assignment: Logistic Regression (10 points)\n",
    "\n",
    "### by Konrad Krawczyk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn is a toolkit that has several datasets built in. You first need to install the toolkit: http://scikit-learn.org/stable/install.html\n",
    "\n",
    "The MNIST dataset that you will be using for this assignment contains images of hand-written digits that are only 8 by 8 pixels, which means the algorithm (logistic regression) should run on every computer.\n",
    "\n",
    "The code in the cell below shows how to work with the digits dataset and  how to visualize it. As you can see the numbers are not very clear in 8x8 pixels images, this means we cannot expect our logistic regression will have a very high classification score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACoCAYAAADw6BWzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x115d86128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=10)\n",
    "\n",
    "#Create two rows with numbers\n",
    "firstrow = np.hstack(digits.images[:5,:,:])\n",
    "secondrow = np.hstack(digits.images[5:10,:,:])\n",
    "\n",
    "plt.gray()\n",
    "plt.axis('off')\n",
    "\n",
    "#Show both rows at the same time\n",
    "plt.imshow(np.vstack((firstrow,secondrow)))\n",
    "#print \"The numbers shown are: \\n\", np.vstack((digits.target[:5], digits.target[5:10]))\n",
    "\n",
    "X, y = digits.data[:1700,:], digits.target[:1700]\n",
    "tX, ty = digits.data[1700:,:], digits.target[1700:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The implementation\n",
    "\n",
    "You have to implement the next three functions and fill in the body of the loop in order to create a correct implementation of logistic regression. Don't change the definitions of the functions and input parameters.\n",
    "\n",
    "(1) Make sure that you do not overfit by keeping track of the score on the test set and implementing a correct stop condition. \n",
    "(2) Systematically pick a learning rate alpha that makes sure the algorithm learns in a smooth and stable manner (show how you do it). \n",
    "(3) Plot how your score on the test set improves over time. My best score was about 85% correct!\n",
    "(4) Write a short summary of what you have done (and why) to accomplish steps (1), (2) and (3).\n",
    "(5) Make sure to comment your code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a prediction function h\n",
    "def prediction_function(x,theta):\n",
    "    z = np.dot(theta.T, x) #theta transpose x, our general hypothesis\n",
    "    g = 1/(1+np.exp(-z)) #function specific for logistic regression\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use the output of that function to compute the cost function J:\n",
    "def cost_function(x_predict,y):\n",
    "    result = y*(np.log(x_predict)) + (1-y)*np.log(1-x_predict)\n",
    "    cost = -sum(result) / np.size(x_predict)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a function that returns the gradient values, given h (x_predict), y and x:\n",
    "\n",
    "#Dimensions: \n",
    "#x_predict: 10,1; y: 10,1; x: 64,1\n",
    "#The output has to be 64,10 - a matrix of gradients per each pixel per each number\n",
    "\n",
    "def compute_gradient(x_predict, y, x):\n",
    "    dtheta = outer((x_predict-y),x)\n",
    "    grad = dtheta / np.size(x)\n",
    "    return np.transpose(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f55ee2a00145>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#Choose a suitable learning rate\n",
    "alpha = 0.003\n",
    "iterations = 25\n",
    "theta = np.zeros((64,10))\n",
    "stop = 0.00001\n",
    "cost = 10\n",
    "\n",
    "#It is important to check that you're not overfitting by testing your prediction on a testset\n",
    "\n",
    "#Training data\n",
    "x = np.reshape(digits.images[:1500],(1500,64))\n",
    "x_test = np.reshape(digits.images[1500:],(297,64))\n",
    "\n",
    "#Test data\n",
    "target = digits.target[:1500]\n",
    "target_test = digits.target[1500:]\n",
    "\n",
    "print(x[j,:]);\n",
    "\n",
    "#THE LOOP:\n",
    "A=[]\n",
    "for i in range(iterations):\n",
    "    for j in range(x.shape[0]):\n",
    "        \n",
    "        x_predict = prediction_function(x[j,:],theta)\n",
    "        y = np.zeros(10)\n",
    "        y[target[j]] = 1\n",
    "        gradient = compute_gradient(x_predict, y, x[j, :])\n",
    "        theta = theta - (alpha*gradient)\n",
    "    A.append(cost_function(x_predict,y))\n",
    "\n",
    "\n",
    "#The function that takes an input and classifies it as a certain digit based on pixel values.\n",
    "def predictor(x, theta):\n",
    "    return(argmax(prediction_function(np.transpose(x), theta)))\n",
    "\n",
    "#Calculate the number of successful digit recognitions in the training set:\n",
    "\n",
    "success = 0\n",
    "for i in range(x.shape[0]):\n",
    "    if predictor(x[i], theta) == target[i]:\n",
    "        success += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Training sample accuracy:\")\n",
    "print((success/x.shape[0]) * 100)\n",
    "\n",
    "#Calculate the number of successful digit recognitions in the testing set:\n",
    "\n",
    "success_test = 0\n",
    "for i in range(x_test.shape[0]):\n",
    "    if predictor(x_test[i], theta) == target_test[i]:\n",
    "        success_test += 1\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print(\"Test sample accuracy:\")\n",
    "print((success_test/x_test.shape[0]) * 100)\n",
    "\n",
    "plt.plot(A)\n",
    "plt.title('Best Score Analysis')\n",
    "plt.xlabel('no. of iterations')\n",
    "plt.ylabel('cost function value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SUMMARY\n",
    "\n",
    "This is a basic implementation of logistic regression for classifying images.\n",
    "It takes 1797 samples of handwritten digits, and maps them onto an image matrix 8 by 8 pixels.\n",
    "Based on expected values of brightness of these pixels, we can compare the image of a given sample of handwriting and, by comparing the values to the average values learned by the set, can recognize the digit that's most likely written on the sample.\n",
    "\n",
    "This algorithm of course does not have 100% accuracy since handwriting styles differ by individual and the writing system of a given country. Also there are some numbers that are generally harder to recognize - for example 7.\n",
    "\n",
    "Nevertheless, this is a framework that can be used for some perhaps more advanced applications in computer vision such as shape recognition, etc.\n",
    "\n",
    "### OVERFIT PREVENTION\n",
    "\n",
    "Since the success rates of prediction within both training and testing sets are similarly high (88% - 97%), we can safely assume that the hypothesis is effective enough in generalizing the score results. Therefore, no additional measures to prevent overfitting are necessary. \n",
    "\n",
    "### LEARNING RATE\n",
    "\n",
    "After trying different rates of alpha, ***0.003*** turns out to be the most accurate one based on the testing set.\n",
    "\n",
    "| Alpha | Training accuracy | Testing accuracy |\n",
    "|---|---|---|\n",
    "| 0.001 | 95.93 | 87.87 |\n",
    "| ***0.003*** | ***96.86*** | ***88.88*** |\n",
    "| 0.01 | 96.93 | 88.21 |\n",
    "| 0.03 | 97.06 | 87.54 |\n",
    "| 0.05 | 96.6 | 86.53 |\n",
    "| 0.1 | 95.0 | 82.82 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
